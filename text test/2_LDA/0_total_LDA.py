import re
import jieba
import jieba.analyse
import pandas as pd
import tqdm
# from gensim import corpora, similarities, models
# from gensim.models import LdaModel
# from gensim.models import CoherenceModel
# import matplotlib.pyplot as plt
import tomotopy as tp


# p_weibo_data = r'E:\Document\MyStrategy\DEDA_fdu24fall_finalproj_group1\weibo-search-master\weibo\ç»“æœæ–‡ä»¶' + '\\'
# p_lda_file = r'E:\Document\MyStrategy\DEDA_fdu24fall_finalproj_group1\weibo-public-opinion-analysis\LDA' + '\\'
# p_out = r'E:\Document\MyStrategy\DEDA_fdu24fall_finalproj_group1\weibo-public-opinion-analysis\output' + '\\'


def clean(line):
    """å¯¹ä¸€ä¸ªæ–‡ä»¶çš„æ•°æ®è¿›è¡Œæ¸…æ´—"""
    rep = ['ã€ã€‘', 'ã€', 'ã€‘', 'ğŸ‘', 'ğŸ¤',
           'ğŸ®', 'ğŸ™', 'ğŸ‡¨ğŸ‡³', 'ğŸ‘', 'â¤ï¸', 'â€¦â€¦â€¦', 'ğŸ°', '...ã€ã€', 'ï¼Œï¼Œ', '..', 'ğŸ’ª', 'ğŸ¤“',
           'âš•ï¸', 'ğŸ‘©', 'ğŸ™ƒ', 'ğŸ˜‡', 'ğŸº', 'ğŸ‚', 'ğŸ™ŒğŸ»', 'ğŸ˜‚', 'ğŸ“–', 'ğŸ˜­', 'âœ§Ù©(ËŠÏ‰Ë‹*)Ùˆâœ§', 'ğŸ¦', 'ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ', '//', 'ğŸ˜Š', 'ğŸ’°', 'ğŸ˜œ', 'ğŸ˜¯',
           '(áƒ¦Ë˜âŒ£Ë˜áƒ¦)', 'âœ§ï¼¼Ù©(ëˆˆà±ªëˆˆ)Ùˆ/ï¼âœ§', 'ğŸŒ', 'ğŸ€', 'ğŸ´',
           'ğŸŒ»', 'ğŸŒ±', 'ğŸŒ±', 'ğŸŒ»', 'ğŸ™ˆ', '(à¸‡â€¢Ì€_â€¢Ì)à¸‡ï¼', 'ğŸ‰‘ï¸', 'ğŸ’©',
           'ğŸ', 'âŠ™âˆ€âŠ™ï¼', 'ğŸ™Š', 'ã€ï¼Ÿ', '+1', 'ğŸ˜„', 'ğŸ™', 'ğŸ‘‡ğŸ»', 'ğŸ“š', 'ğŸ™‡',
           'ğŸ™‹', 'ï¼ï¼ï¼ï¼', 'ğŸ‰', 'ï¼¼(^â–½^)ï¼', 'ğŸ‘Œ', 'ğŸ†’', 'ğŸ»',
           'ğŸ™‰', 'ğŸµ', 'ğŸˆ', 'ğŸŠ', '0371-12345', 'â˜•ï¸', 'ğŸŒ', 'ğŸ˜³', 'ğŸ‘»', 'ğŸ¶', 'ğŸ‘„', '\U0001f92e\U0001f92e', 'ğŸ˜”', 'ï¼‹1', 'ğŸ›€', 'ğŸ¸',
           'ğŸ·', 'â•1',
           'ğŸŒš', 'ï¼šï¼š', 'ğŸ’‰', 'âˆš', 'x', 'ï¼ï¼ï¼', 'ğŸ™…', 'â™‚ï¸', 'ğŸ’Š', 'ğŸ‘‹', 'o(^o^)o', 'mei\u2006sha\u2006shi', 'ğŸ’‰', 'ğŸ˜ª', 'ğŸ˜±',
           'ğŸ¤—', 'å…³æ³¨', 'â€¦â€¦', '(((â•¹Ğ´â•¹;)))', 'âš ï¸', 'Ô¾â€¸Ô¾', 'â›½ï¸', 'ğŸ˜“', 'ğŸµ',
           'ğŸ™„ï¸', 'ğŸŒ•', 'â€¦', 'ğŸ˜‹', '[]', '[', ']', 'â†’_â†’', 'ğŸ’', 'ğŸ˜¨', '&quot;', 'ğŸ˜', 'à¸…Û¶â€¢ï»Œâ€¢â™¡', 'ğŸ˜°', 'ğŸ™ï¸',
           'ğŸ¤§', 'ğŸ˜«', '(à¸‡â€¢Ì€_â€¢Ì)à¸‡', 'ğŸ˜', 'âœŠ', 'ğŸš¬', 'ğŸ˜¤', 'ğŸ‘»', 'ğŸ˜£', 'ï¼š', 'ğŸ˜·', '(*^â–½^)/â˜…*â˜†', 'ğŸ', 'ğŸ”', 'ğŸ˜˜', 'ğŸ‹', '(âœªâ–½âœª)',
           '(âÂ´Ï‰`â)', '1âƒ£3âƒ£', '(^_^)ï¼', 'â˜€ï¸',
           'ğŸ', 'ğŸ˜…', 'ğŸŒ¹', 'ğŸ ', 'â†’_â†’', 'ğŸ™‚', 'âœ¨', 'â„ï¸', 'â€¢', 'ğŸŒ¤', 'ğŸ’“', 'ğŸ”¨', 'ğŸ‘', 'ğŸ˜', 'âŠ™âˆ€âŠ™ï¼', 'ğŸ‘',
           'âœŒ(Ì¿â–€Ì¿\u2009Ì¿Ä¹Ì¯Ì¿Ì¿â–€Ì¿Ì¿)âœŒ',
           'ğŸ˜Š', 'ğŸ‘†', 'ğŸ’¤', 'ğŸ˜˜', 'ğŸ˜Š', 'ğŸ˜´', 'ğŸ˜‰', 'ğŸŒŸ', 'â™¡â™ª..ğ™œğ™¤ğ™¤ğ™™ğ™£ğ™ğ™œğ™ğ™©â€¢Íˆá´—â€¢Íˆâœ©â€§â‚ŠËš', 'ğŸ‘ª', 'ğŸ’°', 'ğŸ˜', 'ğŸ€', 'ğŸ›', 'ğŸ–•ğŸ¼', 'ğŸ˜‚',
           '(âœªâ–½âœª)', 'ğŸ‹', 'ğŸ…', 'ğŸ‘€', 'â™‚ï¸', 'ğŸ™‹ğŸ»', 'âœŒï¸', 'ğŸ¥³', 'ï¿£ï¿£)Ïƒ',
           'ğŸ˜’', 'ğŸ˜‰', 'ğŸ¦€', 'ğŸ’–', 'âœŠ', 'ğŸ’ª', 'ğŸ™„', 'ğŸ£', 'ğŸŒ¾', 'âœ”ï¸', 'ğŸ˜¡', 'ğŸ˜Œ', 'ğŸ”¥', 'â¤', 'ğŸ¼', 'ğŸ¤­', 'ğŸŒ¿', 'ä¸¨', 'âœ…', 'ğŸ¥', 'ï¾‰',
           'â˜€', '5âƒ£âº1âƒ£0âƒ£', 'ğŸš£', 'ğŸ£', 'ğŸ¤¯', 'ğŸŒº',
           'ğŸŒ¸',
           ]
    pattern_0 = re.compile('#.*?#')  # åœ¨ç”¨æˆ·åå¤„åŒ¹é…è¯é¢˜åç§°
    pattern_1 = re.compile('ã€.*?ã€‘')  # åœ¨ç”¨æˆ·åå¤„åŒ¹é…è¯é¢˜åç§°
    pattern_2 = re.compile('è‚ºç‚@([\u4e00-\u9fa5\w\-]+)')  # åŒ¹é…@
    pattern_3 = re.compile('@([\u4e00-\u9fa5\w\-]+)')  # åŒ¹é…@
    # è‚ºç‚@ç¯çƒæ—¶æŠ¥
    pattern_4 = re.compile(u'[\U00010000-\U0010ffff\uD800-\uDBFF\uDC00-\uDFFF]')  # åŒ¹é…è¡¨æƒ…
    pattern_5 = re.compile('(.*?)')  # åŒ¹é…ä¸€éƒ¨åˆ†é¢œæ–‡å­—
    pattern_7 = re.compile('L.*?çš„å¾®åšè§†é¢‘')
    pattern_8 = re.compile('ï¼ˆ.*?ï¼‰')
    # pattern_9=re.compile(u"\|[\u4e00-\u9fa5]*\|")#åŒ¹é…ä¸­æ–‡

    line = line.replace('Oç½‘é¡µé“¾æ¥', '')
    line = line.replace('-----', '')
    line = line.replace('â‘ ', '')
    line = line.replace('â‘¡', '')
    line = line.replace('â‘¢', '')
    line = line.replace('â‘£', '')
    line = line.replace('>>', '')
    line = re.sub(pattern_0, '', line, 0)  # å»é™¤è¯é¢˜
    line = re.sub(pattern_1, '', line, 0)  # å»é™¤ã€ã€‘
    line = re.sub(pattern_2, '', line, 0)  # å»é™¤@
    line = re.sub(pattern_3, '', line, 0)  # å»é™¤@
    line = re.sub(pattern_4, '', line, 0)  # å»é™¤è¡¨æƒ…
    # line = re.sub(pattern_5, '', line, 0)  # å»é™¤ä¸€éƒ¨åˆ†é¢œæ–‡å­—
    line = re.sub(pattern_7, '', line, 0)
    line = re.sub(pattern_8, '', line, 0)
    line = re.sub(r'\[\S+\]', '', line, 0)  # å»é™¤è¡¨æƒ…ç¬¦å·

    for i in rep:
        line = line.replace(i, '')
    return line


def stopwordslist(filepath):
    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]
    return stopwords#åœç”¨è¯


def synwordslist(filepath):
    syn = dict()
    with open(filepath, 'r',encoding='utf-8') as f:
        for line in f:
            synword = line.strip().split("\t")
            num = len(synword)
            for i in range(1, num):
                syn[synword[i]] = synword[0]
    return syn  # è¿‘ä¹‰è¯å…¸


def seg_sentence(sentence, stopwords, synwords):
    sentence = re.sub(u'[0-9\.]+', u'', sentence)

    sentence_seged =jieba.cut(sentence.strip(),cut_all=False,use_paddle=10)#é»˜è®¤ç²¾ç¡®æ¨¡å¼
    output_list = []
    for word in sentence_seged:
        if word not in stopwords and word.__len__()>1 and word != '\t':
            if word in synwords.keys():#å¦‚æœæ˜¯åŒä¹‰è¯
                word = synwords[word]
            if word is not None:
                word.encode('utf-8')
                output_list.append(word)
    return output_list


def get_raw_data():
    p_in = r'D:/Desktop/grade 1/DEDA/HW/text analysis/text test/content data/æ¬§æ´²èˆªçº¿.xlsx'
    data = pd.read_excel(p_in)
    return data


def split_words(row, stopwords, synwords):
    content = row['å¾®åšå†…å®¹']
    sentence = clean(content)
    return seg_sentence(sentence, stopwords, synwords)

#
# def deal_lda(train, p_out):
#     #è¾“å…¥trainï¼Œè¾“å‡ºè¯å…¸,textså’Œå‘é‡
#     id2word = corpora.Dictionary(train)     # Create Dictionary
#     texts = train                           # Create Corpus
#     corpus = [id2word.doc2bow(text) for text in texts]   # Term Document Frequency
#
#     #ä½¿ç”¨tfidf
#     tfidf = models.TfidfModel(corpus)
#     corpus = tfidf[corpus]
#
#     id2word.save(p_out+'deerwester.dict') #ä¿å­˜è¯å…¸
#     corpora.MmCorpus.serialize(p_out+'deerwester.mm', corpus)#ä¿å­˜corpus
#
#     return id2word,texts,corpus
#
# def run(corpus_1,id2word_1,num,texts):
#     #æ ‡å‡†LDAç®—æ³•
#     lda_model = LdaModel(corpus=corpus_1,
#                          id2word=id2word_1,
#                         num_topics=num,
#                        passes=60,
#                        alpha=(50/num),
#                        eta=0.01,
#                        random_state=42)
#     # num_topicsï¼šä¸»é¢˜æ•°ç›®
#     # passesï¼šè®­ç»ƒä¼¦æ¬¡
#     # numï¼šæ¯ä¸ªä¸»é¢˜ä¸‹è¾“å‡ºçš„termçš„æ•°ç›®
#     #è¾“å‡ºä¸»é¢˜
#     #topic_list = lda_model.print_topics()
#     #for topic in topic_list:
#         #print(topic)
#     # å›°æƒ‘åº¦
#     perplex=lda_model.log_perplexity(corpus_1)  # a measure of how good the model is. lower the better.
#     # ä¸€è‡´æ€§
#     coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word_1, coherence='c_v')
#     coherence_lda = coherence_model_lda.get_coherence()
#     #print('\nä¸€è‡´æ€§æŒ‡æ•°: ', coherence_lda)   # è¶Šé«˜è¶Šå¥½
#     return lda_model,coherence_lda,perplex
#
#
# def compute_coherence_values(dictionary, corpus, texts,start, limit, step):
#     """
#     Compute c_v coherence for various number of topics
#
#     Parameters:
#     ----------
#     dictionary : Gensim dictionary
#     corpus : Gensim corpus
#     texts : List of input texts
#     limit : Max num of topics
#
#     Returns:
#     -------
#     model_list : List of LDA topic models
#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics
#     """
#     coherence_values = []
#     perplexs=[]
#     model_list = []
#     for num_topic in range(start, limit, step):
#         #æ¨¡å‹
#         lda_model,coherence_lda,perplex=run(corpus,dictionary,num_topic,texts)
#         #lda_model = LdaModel(corpus=corpus,num_topics=num_topic,id2word=dictionary,passes=50)
#         model_list.append(lda_model)
#         perplexs.append(perplex)#å›°æƒ‘åº¦
#         #ä¸€è‡´æ€§
#         #coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')
#         #coherence_lda = coherence_model_lda.get_coherence()
#         coherence_values.append(coherence_lda)
#
#     return model_list, coherence_values,perplexs
#
#
# def show_1(dictionary, corpus, texts, start, limit, step):
#     # ä» 5 ä¸ªä¸»é¢˜åˆ° 30 ä¸ªä¸»é¢˜ï¼Œæ­¥é•¿ä¸º 5 é€æ¬¡è®¡ç®—ä¸€è‡´æ€§ï¼Œè¯†åˆ«æœ€ä½³ä¸»é¢˜æ•°
#     model_list, coherence_values, perplexs = compute_coherence_values(dictionary, corpus, texts, start, limit, step)
#     # è¾“å‡ºä¸€è‡´æ€§ç»“æœ
#     n = 0
#     for m, cv in zip(perplexs, coherence_values):
#         print("ä¸»é¢˜æ¨¡å‹åºå·æ•°", n, "ä¸»é¢˜æ•°ç›®", (n + 4), "å›°æƒ‘åº¦", round(m, 4), " ä¸»é¢˜ä¸€è‡´æ€§", round(cv, 4))
#         n = n + 1
#     # æ‰“å°æŠ˜çº¿å›¾
#     x = list(range(start, limit, step))
#     # å›°æƒ‘åº¦
#     plt.plot(x, perplexs)
#     plt.xlabel("Num Topics")
#     plt.ylabel("perplex  score")
#     plt.legend(("perplexs"), loc='best')
#     plt.show()
#     # ä¸€è‡´æ€§
#     plt.plot(x, coherence_values)
#     plt.xlabel("Num Topics")
#     plt.ylabel("Coherence score")
#     plt.legend(("coherence_values"), loc='best')
#     plt.show()
#
#     return model_list
#

def find_k(docs, min_k, max_k, min_df):
    # min_df è¯è¯­æœ€å°‘å‡ºç°åœ¨2ä¸ªæ–‡æ¡£ä¸­
    import matplotlib.pyplot as plt
    scores = []
    for k in range(min_k, max_k):
        # seedéšæœºç§å­ï¼Œä¿è¯åœ¨å¤§é‚“è¿™é‡Œè¿è¡Œç»“æœä¸ä½ è¿è¡Œçš„ç»“æœä¸€æ ·
        mdl = tp.LDAModel(min_df=min_df, k=k, seed=555)
        for words in docs:
            if words:
                mdl.add_doc(words)
        mdl.train()
        coh = tp.coherence.Coherence(mdl)
        scores.append(coh.get_score())

    # x = list(range(min_k, max_k - 1))  # åŒºé—´æœ€å³ä¾§çš„å€¼ã€‚æ³¨æ„ï¼šä¸èƒ½å¤§äºmax_k
    # print(x)
    # print()
    plt.plot(range(min_k, max_k), scores)
    plt.xlabel("number of topics")
    plt.ylabel("coherence")
    plt.show()




def main():
    raw_data = get_raw_data()
    jieba.load_userdict('D:/Desktop/grade 1/DEDA/HW/text analysis/text test/2_LDA/è‡ªå»ºè¯è¡¨.txt')#åŠ è½½è‡ªå»ºè¯è¡¨
    stopwords = stopwordslist('D:/Desktop/grade 1/DEDA/HW/text analysis/text test/2_LDA/åœç”¨è¯è¡¨.txt')  # è¿™é‡ŒåŠ è½½åœç”¨è¯çš„è·¯å¾„
    synwords = synwordslist('D:/Desktop/grade 1/DEDA/HW/text analysis/text test/2_LDA/è¿‘ä¹‰è¯è¡¨.txt')#è¿™é‡ŒåŠ è½½è¿‘ä¹‰è¯çš„è·¯å¾„
    sentence_split = []
    len(raw_data)
    for _, row in tqdm.tqdm(raw_data.iterrows()):
        if isinstance(row['å¾®åšå†…å®¹'], str):
            sentence_split.append(split_words(row, stopwords, synwords))
    # id2word, texts, corpus = deal_lda(sentence_split, p_out)
    # lda_model,coherence_lda,perplex = run(corpus, id2word, 3, texts)
    # topic_list = lda.print_topics()
    # model_list = show_1(id2word, corpus, texts, 4, 16, 1)  # æ‰¾å‡ºå›°æƒ‘åº¦å’Œä¸»é¢˜ä¸€è‡´æ€§æœ€ä½³çš„ï¼Œæœ€å¥½æ˜¯ä¸è¶…è¿‡20ä¸ªä¸»é¢˜æ•°,10ä¸ªä¸ºå®œ
    # n = input('è¾“å…¥æŒ‡å®šæ¨¡å‹åºå·ï¼Œä»¥0ä¸ºç¬¬ä¸€ä¸ª: ')  # è¿˜æ˜¯éœ€è¦æ‰‹åŠ¨ï¼Œæƒè¡¡æ¯”è¾ƒ
    # optimal_model = choose(model_list, int(n))
    pd.to_pickle(sentence_split, 'D:/Desktop/grade 1/DEDA/HW/text analysis/text test/content data/coastline_sentence_split.pkl')
    find_k(sentence_split, 2, 2, 2)

    import tomotopy as tp

    mdl = tp.LDAModel(k=5, min_df=2, seed=555)
    for words in sentence_split:
        if words:
            mdl.add_doc(words=words)
    mdl.train()
    for k in range(mdl.k):
        print('Top 10 words of topic #{}'.format(k))
        print(mdl.get_topic_words(k, top_n=10))
        print('\n')
    mdl.summary()


    import pyLDAvis
    import numpy as np

    #è·å–pyldaviséœ€è¦çš„å‚æ•°
    topic_term_dists = np.stack([mdl.get_topic_word_dist(k) for k in range(mdl.k)])
    doc_topic_dists = np.stack([doc.get_topic_dist() for doc in mdl.docs])
    doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)
    doc_lengths = np.array([len(doc.words) for doc in mdl.docs])
    vocab = list(mdl.used_vocabs)
    term_frequency = mdl.used_vocab_freq


    prepared_data = pyLDAvis.prepare(
        topic_term_dists,
        doc_topic_dists,
        doc_lengths,
        vocab,
        term_frequency,
        # start_index=0, # tomotopyè¯é¢˜idä»0å¼€å§‹ï¼ŒpyLDAvisè¯é¢˜idä»1å¼€å§‹
        sort_topics=False #æ³¨æ„ï¼šå¦åˆ™pyLDAvisä¸tomotopyå†…çš„è¯é¢˜æ— æ³•ä¸€ä¸€å¯¹åº”ã€‚
    )


    #å¯è§†åŒ–ç»“æœå­˜åˆ°htmlæ–‡ä»¶ä¸­
    pyLDAvis.save_html(prepared_data, 'D:/Desktop/grade 1/DEDA/HW/text analysis/text test/content data/coastline_ldavis.html')

    #notebookä¸­æ˜¾ç¤º
    pyLDAvis.display(prepared_data)






if __name__ == "__main__":
    main()